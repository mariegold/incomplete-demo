---
title: Statistical Computing
subtitle: Coursework B
author: Marie Biolkova (s1653813)
output: pdf_document
---

Data Description
=====
After several archaeological excavations of a gravesite, a total of 493 femurs (256 left and 237 right) were found. The goal is to figure out how many persons were likely buried at the gravesite. 
We assume the number of left ($y_1$) and right ($y_2$) femurs are two independent observations from a Bin($N,\phi$) distribution, where $N$ is the total number of people buried and $\phi$ is the probability of finding a femur. We will treat $N$ as a continuous parameter.

Additional Code
=====
Import provided code:
```{r echo = FALSE}
setwd("~/Desktop/Statistical Computing/CWB")
options(xtable.comment = FALSE)
set.seed(72)
```
```{r}
source('CWB2019code.R')
```

To print $2\times 2$ matrices or simple tables, I will make use of the following function.

```{r results="asis", warning=FALSE}
library(xtable) 

printmatrix <- function(data, cap, digits, sc = FALSE, coln, rown) {
table <- data.frame(format(data, digits = digits, scientific = sc))
colnames(table) <- coln
rn <- FALSE    
if (rown != FALSE) {     # checks if we want to give names to rows
  rownames(table) <- rown 
  rn <- TRUE
  }

xtable::print.xtable(
xtable(table, caption = cap, align = "ccc"),
hline.after = NULL,
booktabs = TRUE,
sanitize.text.function=function(x){x},
include.rownames = rn)
}
```

Question 1
======
Using the change of variables $\theta = \log{(\phi)} - \log{(1-\phi)}$, the negated log-likelihood is
\vspace{-0.5cm}
\begin{align*} l(N,\theta) &= \log\Gamma(y_1 +1)+ \log\Gamma(y_2 +1) \\ 
&+ \log\Gamma(N - y_1 +1)+\log\Gamma(N -y_2 +1)-2\log\Gamma(N +1) \\ & +2N\log(1+e^\theta)-(y_1 + y_2)\theta. \end{align*}

We implement it in `R` using the `lgamma()` function to evaluate $\log\Gamma(x)$:
```{r}
negloglike <- function(param, Y) {
  N <- param[1]
  theta <- param[2]
  if (N >= max(Y)) {
   nlog <- (sum(lgamma(Y+1)) + sum(lgamma(N-Y+1)) 
           - 2*lgamma(N+1) + 2*N*log(1+exp(theta)) 
           - sum(Y)*theta)
   nlog
  } else {
    +Inf
  }
}
```

The maximum likelihood estimate of $N$ and $\theta$ can be found as follows:

```{r}
Y <- c(256, 237) # observed data
mle <- optim(c(max(Y) + 1, 0), negloglike, Y = Y)
theta_hat <- mle$par[2]
N_hat <- mle$par[1]
phi_hat <- ilogit(theta_hat)
cat(sprintf("MLE for theta: %.2f
            \nMLE for N: %.2f
            \nMLE for phi: %.2f", theta_hat ,N_hat, phi_hat))
```

Let us find the Hessian $\mathbf{H}$ of $l(N,\theta)$ at the mode $(\hat{N}, \hat{\theta})$ and its inverse:
```{r  results = 'asis', warning = FALSE, message=FALSE}
hessian <- optimHess(mle$par, negloglike, Y = Y)
hess_inv <- solve(hessian)  # finds inverse

printmatrix(hessian, cap = 'Hessian at the mode.', digits = 2, 
            coln = c('$\\frac{\\partial}{\\partial{N}}$',
                     '$\\frac{\\partial}{\\partial{\\theta}}$'),
            rown = c('$\\frac{\\partial}{\\partial{N}}$',
                     '$\\frac{\\partial}{\\partial{\\theta}}$'))
printmatrix(hess_inv, cap = 'Inverse Hessian at the mode.', digits = 3, 
            coln = c('$\\frac{\\partial}{\\partial{N}}$',
                     '$\\frac{\\partial}{\\partial{\\theta}}$'),
            rown = c('$\\frac{\\partial}{\\partial{N}}$',
                     '$\\frac{\\partial}{\\partial{\\theta}}$'))
```

Therefore \[\mathbf{H}(\hat{N}, \hat{\theta})= \begin{pmatrix} 0.009 & 1.27 \\ 1.27 & 179.73 \end{pmatrix}, \quad \mathbf{H^{-1}}(\hat{N}, \hat{\theta})= \begin{pmatrix} 44943.91 & -317.80 \\ -317.80 & 2.25 \end{pmatrix}.\]


We now calculate the 95% confidence interval for $N$ based on Normal approximation with joint covariance matrix $\mathbf{H}^{-1}$:
```{r}
var_N <- hess_inv[1,1]   # extracting variance of N from the covariance matrix
CI_N <- qnorm(c(0.025,0.975), N_hat, sqrt(var_N))
cat(sprintf("95%% confidence interval for N: [%.2f, %.2f]", CI_N[1], CI_N[2]))
```

This is clearly a huge confidence interval with some values included not being reasonable. We obviously cannot have a negative number of dead people, furthermore, we know that there must have been at least 256 persons buried at the gravesite (assuming each person had both legs, i.e. there were no cripples with only one leg). Hence the interval does not respect the lower bound $\max(y_1,y_2)$ imposed. The reason the interval is so big is because it is based on a single observation which means the variance is very large (see first entry in $\mathbf{H}^{-1}$).

Question 2
======
Differentiating $l(N,\theta)$, we derive the following expressions:
\begin{align*} \frac{\partial l}{\partial \theta} &= \frac{2Ne^\theta}{1+e^\theta} - (y_1+y_2) \\
 \frac{\partial l}{\partial N} &= \Psi(N-y_1+1) + \Psi(N-y_2+1) - 2\Psi(N+1) + 2\log{(1+e^\theta)}\\
 \frac{\partial^2 l}{\partial \theta^2} &= \frac{2Ne^\theta(1+e^\theta)-2Ne^{2\theta}}{(1+e^\theta)^2} = \frac{2Ne^\theta}{(1+e^\theta)^2} \\
 \frac{\partial^2 l}{\partial \theta \partial N} &= \frac{2e^\theta}{1+e^\theta} \\
 \frac{\partial^2 l}{\partial N^2} &= \Psi'(N-y_1+1) + \Psi'(N-y_2+1) - 2\Psi'(N+1)\end{align*}
 where $\Psi(x) = \frac{d \log\Gamma{(x)}}{d x}$ is the digamma function. The following function constructs the Hessian matrix for $l(N, \theta)$. We can verify it gives the same (or similar) results as `optimHess` in Question 1 by evaluating it at the mode (see Table 3). 
 We will use to find the element-wise relative differences of the resulting Hessian for $(\hat{N},\hat{\theta})$ with the one obtained from `optimHess()`. The results are shown in Table 4. 
```{r results = 'asis', warning = FALSE, message=FALSE}
myhessian <- function(param, Y) {
  N <- param[1]
  theta <- param[2]
  hessian <- matrix(c(0,0,0,0), nrow = 2)    # initializes matrix
  hessian[1,1] <- sum(psigamma(N-Y+1,1))-2*psigamma(N+1,1)
  hessian[1,2] <- (2*exp(theta))/(1+exp(theta))
  hessian[2,1] <- (2*exp(theta))/(1+exp(theta))
  hessian[2,2] <- (2*N*exp(theta))/(1+exp(theta))^2
  hessian
}
my_hess <- myhessian(mle$par, Y)

printmatrix(my_hess, 
            cap = 'Hessian at the mode computed using derived expressions.', 
            digits = 2, 
            coln = c('$\\frac{\\partial}{\\partial{N}}$',
                     '$\\frac{\\partial}{\\partial{\\theta}}$'),
            rown = c('$\\frac{\\partial}{\\partial{N}}$',
                     '$\\frac{\\partial}{\\partial{\\theta}}$'))

# relative differences per element
rel_diff_1 <- (my_hess[1,1]- hessian[1,1])/my_hess[1,1]
rel_diff_2 <- (my_hess[1,2]- hessian[1,2])/my_hess[1,2]
rel_diff_3 <- (my_hess[2,1]- hessian[2,1])/my_hess[2,1]
rel_diff_4 <- (my_hess[2,2]- hessian[2,2])/my_hess[2,2]
rel_diff <- matrix(c(rel_diff_1,rel_diff_2,rel_diff_3,rel_diff_4), nrow = 2)
```

```{r results="asis", warning=FALSE}
printmatrix(rel_diff, 
            cap = 'Relative differences in the two evaluations of Hessian.', 
            digits = 3, sc = TRUE,
            coln = c('$\\frac{\\partial}{\\partial{N}}$',
                     '$\\frac{\\partial}{\\partial{\\theta}}$'),
            rown = c('$\\frac{\\partial}{\\partial{N}}$',
                     '$\\frac{\\partial}{\\partial{\\theta}}$'))
```

The biggest relative difference corresponds to the first entry. The reason could be that its second derivative has the most complicated expression (longest and involving the trigamma function) out of all entries.

```{r results="asis", warning=FALSE}
# bound for second order difference approximations
e_0 <- .Machine$double.eps   # machine precision number
h <- 0.0001
L0 <- negloglike(c(N_hat, theta_hat), Y)
L1 <- sum(psigamma(N_hat-Y+1,1))-2*psigamma(N_hat+1,1)
L4 <- abs(sum(psigamma(N_hat - Y + 1, 3)) - 2 * psigamma(N_hat + 1, 3))
bound <- ((e_0*(4*L0 + 2*abs(theta_hat)*L1))/(h^2)) + ((h^2 * L4)/12)
cat('Bound for 2nd order difference approximations: ', signif(bound, 3))

# absolute differnces per element
abs_diff_obs <- abs(hessian-my_hess)
printmatrix(abs_diff_obs, 
            cap = 'Absolute differences in the two evaluations of  Hessian.', 
            digits = 3, sc = TRUE,
            coln = c('$\\frac{\\partial}{\\partial{N}}$',
                     '$\\frac{\\partial}{\\partial{\\theta}}$'),
            rown = c('$\\frac{\\partial}{\\partial{N}}$',
                     '$\\frac{\\partial}{\\partial{\\theta}}$'))
```
We have found that, assuming `optimHess` used $h = 0.0001$, the bound for second order difference approximations is $6.52 \times 10^{-7}$. The absolute difference between `optimHess` and `myhessian` for the evaluation of $\frac{\partial^2 l}{\partial N^2}$ is $8.77 \times 10^{-8}$ (see Table 5). This is smaller than the bound, hence the bound for the second order differences is respected for $\frac{\partial^2 l}{\partial N^2}$.

```{r results="asis", warning=FALSE}
# computational cost of myhessian and optimHess
cost <- microbenchmark::microbenchmark(myhessian(mle$par, Y),
                                       optimHess(mle$par, negloglike, Y = Y))
xtable(summary(cost), 
       caption = 'Computational costs of the numerical Hessians from optimHess() and myhessian().', 
       digits = 2)
```

Table 6 displays the computational costs for each of the Hessian evaluations and these are visualised in Figure 1 (note the $x$ axis is scaled logarithmically). Due to a large variance in the timings, let us consider the median (or quantiles). Running each function 100 times, `myhessian` is more than 8 times faster than `optimHess`. As opposed to `optimHess`, `myhessian` is already provided with expressions for the derivatives and only needs to evaluate them, therefore it is much faster. 

\pagebreak

```{r warning=FALSE, message=FALSE, fig.cap = "Timings for `myhessian` and `optimHess`.", fig.height = 2.5}
# visualisation of computational cost of the two functions
library(ggplot2)
p <- autoplot(cost)
p + ggtitle("Running times of Hessian evaluations")
```

Question 3
======
The function below produces $J$ parametric bootstrap samples of parameter estimates for the model in Question 1.
```{r}
arch_boot <- function(param, J) {
  # get parameters and initialise output variable
  boot_mle <- matrix(0, 2, nrow = J)
  N <- floor(param[1])  # needs to be an integer
  phi <- ilogit(param[2])
  # generate random data Y 
  boot_sample <- matrix(rbinom(J*2, N, phi), nrow = J)
  # obrain J bootstrap parameter estimates
   for (j in 1:J) {
    Y <- boot_sample[j,]
    mle <- optim(c(2*max(Y), 0), negloglike, Y = Y)
    boot_mle[j,] <- mle$par
  }
  boot_mle
}
```

We use it to estimate the bias and standard deviations of the estimators for $N$ and $\theta$.

```{r}
J <- 10000
boot <- arch_boot(c(N_hat, theta_hat), J)  # parametric bootstrap samples 

N_boot <- boot[,1]
theta_boot <- boot[,2]

bias_N <- mean(N_boot - N_hat)
bias_theta <- mean(theta_boot - theta_hat)

sd_N <- sd(boot[,1])
sd_theta <- sd(boot[,2])

cat(sprintf("Estimate of bias of N hat: %.2f
            \nEstimate of standard deviation of N hat: %.2f
            \nEstimate of bias of theta hat: %.2f
            \nEstimate of standard deviation of theta hat: %.2f", 
            bias_N, sd_N, bias_theta, sd_theta))
```

The estimate of the bias is quite high, and the bias is positive, so it tends to overestimate the parameter. Note that we need the Bootstrap Principle assumption $\mathbb{E}(\hat{\theta}-\theta_{\text{true}})=\mathbb{E}(\hat{\theta}^{(j)}-\hat{\theta})$ to estimate the bias (as $\theta_{\text{true}}$ is unknown).
One thing to point out is that we did not check that `optim` converged in the `arch_boot` function, so it will simply output the value at which `optim` reached the maximum number of allowed iterations. But that is not necessarily a reliable value. However, since these values are perfecly valid pairs of data, replacing them with new ones (randomly generated) would probably introduce an even bigger bias than ignoring the non-convergence. One might want to consider increasing the number of iterations for `optim` to address this issue. 

While most values are in the range [200, 500], in Figure 2 we see that the bootstrap samples for $N$ can take values up to over 20000. Hence the distribution is positively skewed.

As for the variance of $N$, it is clearly very big. We are dealing with an extreme case where we are provided with only one pair of sample data. Bootstrap is a method that tries to make the most out of so little information but often at the price of large variance. 

If we imposed the lower bound max$(y_1,y_2)$ and maybe knew a bit more about the demographics in Gotland at given period, we should be able to make our estimates more accurate.


```{r warning=FALSE, message=FALSE, fig.cap = "Bootstrap distribution of N.", fig.height = 6}
# visualisation of the distribution of N 
b <- data.frame(boot)
colnames(b) <- c('N','theta')
p1 <- ggplot(data = b) +
  geom_point(aes(x = 1:length(N), y = N)) +
  xlab('Index') +
  ylab('N') +
  ggtitle('Distribution of N')
p2 <- ggplot(b, aes(x=N)) + geom_histogram()
p3 <- ggplot(b, aes(x=log(N))) + geom_histogram()

library(ggpubr)
ggarrange(p1, ggarrange(p2, p3, ncol = 2), nrow = 2)                                 
```

\vspace{2cm}

Assuming that the Bootstrap Principle holds on the $\log{(N)}$ and $\theta = \text{logit}(\phi)$ scales, the bootstrap confidence intervals for $N$ and $\phi$ are calculated as follows.
```{r}
# using the invariance property of MLEs, construct CIs on log / logit
# scales and transform them back using exp / ilogit
boot_CI_phi <- ilogit(theta_hat - quantile(boot[,2] - theta_hat, 
                                           probs = c(0.975, 0.025)))
boot_CI_N <- exp(log(N_hat) - quantile(log(boot[,1]) - log(N_hat), 
                                       probs = c(0.975, 0.025))) 
cat(sprintf("Bootstrap 95%% confidence interval for N: [%.2f, %.2f]
            \nBootstrap 95%% confidence interval for phi: [%.7f, %.4f]", 
            boot_CI_N[1], boot_CI_N[2], boot_CI_phi[1], boot_CI_phi[2]))
```

The confidence interval for $\phi$ covers nearly all range of possible values, so the data are not very informative about the parameter. On the other hand for $N$, the CI has become smaller compared to the one based on Normal approximation in Question 1. Bias deals with the problem of having negative values of $N$ by shifting the lower bound to a positive value. Simultaneously, although variance increased dramatically, it is due to the skewed nature of the distribution and since we are no longer assuming normal distribution of the data, the bootstrap CI upper bound is smaller. 



Question 4
======
First let us explain the steps in `cwb4 scores`. This function takes inputs `data_list` (list of dataframes split into train and test set) and `K` (number of folds for cross validation). It does the following:

- *Step 1*: Generates a vector of the same length as the training set, containing indices 1,...,$K$ defining a random splitting into $K$ subsets. It is saved in the variable `the_splits`.
- *Step 2*: Creates a `data.frame` named `scores_cvk` with $K$ cross-validated scores in each column for columns named `SE`, `DS`, and `Brier` (SE and DS for temperature predictions, and Brier for the event that it was freezing). The folds for cross-validation are defined by `the_splits`. We train a weather model with 2 harmonic functions. 
This is done by calling the function `cvk_do_all` which takes as input the training part of the data, `the_splits` vector from Step 1 and a model formula with 2 harmonic functions suitable for `lm()` constructed by the function `make_formula`. The data is split into $K$ folds using `the_splits` as indices - each of them is used for validation once and for training $K-1$ times. `train_and_validate` is used to obtain the scores for each $K$. 
- *Step 3*: Calculates the mean scores for the test set. This time the model is trained with the entire training set and subsequently evaluated on the test set. Again, we use a weather model with 2 harmonic functions. The last step is averaging the scores and we wish to get the overall average scores, hence `by.ID = FALSE`. The output is of the same form as `scores_cvk` and is saved in variable `scores_test`.
- *Step 4*: Finally the last step puts the following in a list, and it is also the final output of `cwb4 scores`:
    * Mean of the $K$ cross-validated scores `scores_cvk`, separately for each score type, as `cvk_mean`.
    * Standard error of the $K$ cross-validated scores `scores_cvk`, separately for each score type, as `cvk_std_err`.
    * Mean scores for the test set, i.e. `scores_test`.

We generate 20 bootstrap samples of the scores, for $K = 10$:
```{r results='asis'}
temp_data <- read_TMIN_data(path = "~/Desktop/Statistical Computing/CWA")
J <- 20
K <- 10
# initialise data frames
S_boot_train <- data.frame(SE = numeric(J),
                           DS = numeric(J),
                           Brier = numeric(J))
S_boot_test <- data.frame(SE = numeric(J),
                          DS = numeric(J),
                          Brier = numeric(J))

# bootstrap of scores
for (i in 1:J) {
  resample <- data_list_resample(temp_data)
  S_boot_train[i,] <- cwb4_scores(resample, K)$cvk_mean
  S_boot_test[i,] <- cwb4_scores(resample, K)$test
}

xtable(head(S_boot_train), digits = 4, 
       caption = 'Bootstrap samples of the training scores.')
xtable(head(S_boot_test), digits = 4, 
       caption = 'Bootstrap samples of the test scores.')
```

Sample means and standard deviations for each of the three score types, separately for the training and test scores:

```{r results='asis'}
trn <- data.frame(Mean = colMeans(S_boot_train), SD = apply(S_boot_train, 2, sd))
tst <- data.frame(Mean = colMeans(S_boot_test), SD = apply(S_boot_test, 2, sd))

xtable(trn, digits = 4, 
       caption = 'Sample means and standard deviations for the training scores.')
xtable(tst, digits = 4, 
       caption = 'Sample means and standard deviations for the test scores.')
```

```{r results='asis'}
# bias and sd estimates of cross-validation scores
bias_sd <- data.frame(Bias = t(mean_score(S_boot_train - S_boot_test)), 
                      SD = apply(S_boot_train - S_boot_test, 2, sd))
xtable(bias_sd, digits = 5, 
       caption = 'Estimates of bias and standard deviation of 
                  the cross-validation score from training data.')
```

The above code estimates the bias $\text{E}[\hat{S}_0^{\text{train}(K)} - S^{\text{test}}]$ and standard deviation of $\hat{S}_0^{\text{train}(K)}$[^1]. The results can be found in Table 11. The bias is positive for all scores (biggest for SE and smallest for Brier) which means that the test scores are on average smaller than the cross-validation scores. This can happen if we apply the model to the original data because the model captures the patterns in the data almost perfectly. However, notice that the score estimates are dominated by variance - standard deviation is approximately 10 times higher than the bias in each case. In fact, if we run the code multiple times we would see that the bias can be negative sometimes, meaning the test scores are higher than the cross-validation scores. Randomness causes the cross-validation scores to vary depending on how the data is split, and this creates a small bias which can be either negative or positive.


[^1]: Note that the standard deviation of $[\hat{S}_0^{\text{train}(K)} - S^{\text{test}}]$ is equal to the standard devation of $\hat{S}_0^{\text{train}(K)}$ since $S^{\text{test}}$ is a constant.
 